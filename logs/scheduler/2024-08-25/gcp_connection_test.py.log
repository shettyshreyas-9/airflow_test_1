[2024-08-25T10:42:32.955+0000] {processor.py:186} INFO - Started process (PID=4891) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:42:32.957+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T10:42:32.962+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:42:32.961+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:42:33.030+0000] {processor.py:925} INFO - DAG(s) 'test_gcp_connection' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:42:33.237+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:42:33.236+0000] {override.py:1858} INFO - Created Permission View: can edit on DAG:test_gcp_connection
[2024-08-25T10:42:33.258+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:42:33.258+0000] {override.py:1858} INFO - Created Permission View: can delete on DAG:test_gcp_connection
[2024-08-25T10:42:33.274+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:42:33.274+0000] {override.py:1858} INFO - Created Permission View: can read on DAG:test_gcp_connection
[2024-08-25T10:42:33.276+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:42:33.275+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T10:42:33.296+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:42:33.296+0000] {dag.py:3234} INFO - Creating ORM DAG for test_gcp_connection
[2024-08-25T10:42:33.313+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:42:33.313+0000] {dag.py:4138} INFO - Setting next_dagrun for test_gcp_connection to 2024-08-20 00:00:00+00:00, run_after=2024-08-20 00:00:00+00:00
[2024-08-25T10:42:33.347+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.409 seconds
[2024-08-25T10:43:04.171+0000] {processor.py:186} INFO - Started process (PID=4903) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:43:04.173+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T10:43:04.179+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:43:04.179+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:43:04.210+0000] {processor.py:925} INFO - DAG(s) 'test_gcp_connection' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:43:04.246+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:43:04.245+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T10:43:04.268+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:43:04.268+0000] {dag.py:4138} INFO - Setting next_dagrun for test_gcp_connection to 2024-08-20 00:00:00+00:00, run_after=2024-08-20 00:00:00+00:00
[2024-08-25T10:43:04.290+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.127 seconds
[2024-08-25T10:43:34.686+0000] {processor.py:186} INFO - Started process (PID=4919) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:43:34.696+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T10:43:34.719+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:43:34.718+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:43:34.784+0000] {processor.py:925} INFO - DAG(s) 'test_gcp_connection' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:43:34.875+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:43:34.873+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T10:43:34.953+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:43:34.952+0000] {dag.py:4138} INFO - Setting next_dagrun for test_gcp_connection to None, run_after=None
[2024-08-25T10:43:35.022+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.369 seconds
[2024-08-25T10:44:05.292+0000] {processor.py:186} INFO - Started process (PID=4931) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:44:05.294+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T10:44:05.297+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:44:05.297+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:44:05.355+0000] {processor.py:925} INFO - DAG(s) 'test_gcp_connection' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:44:05.391+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:44:05.390+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T10:44:05.423+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:44:05.423+0000] {dag.py:4138} INFO - Setting next_dagrun for test_gcp_connection to None, run_after=None
[2024-08-25T10:44:05.462+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.177 seconds
[2024-08-25T10:44:35.700+0000] {processor.py:186} INFO - Started process (PID=4943) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:44:35.702+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T10:44:35.708+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:44:35.707+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:44:35.766+0000] {processor.py:925} INFO - DAG(s) 'test_gcp_connection' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:44:35.809+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:44:35.808+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T10:44:35.845+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:44:35.845+0000] {dag.py:4138} INFO - Setting next_dagrun for test_gcp_connection to None, run_after=None
[2024-08-25T10:44:35.877+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.187 seconds
[2024-08-25T10:45:06.814+0000] {processor.py:186} INFO - Started process (PID=4956) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:45:06.817+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T10:45:06.820+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:45:06.820+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:45:06.852+0000] {processor.py:925} INFO - DAG(s) 'test_gcp_connection' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:45:06.886+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:45:06.885+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T10:45:06.911+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:45:06.910+0000] {dag.py:4138} INFO - Setting next_dagrun for test_gcp_connection to None, run_after=None
[2024-08-25T10:45:06.941+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.132 seconds
[2024-08-25T10:45:37.102+0000] {processor.py:186} INFO - Started process (PID=4968) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:45:37.104+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T10:45:37.107+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:45:37.106+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:45:37.140+0000] {processor.py:925} INFO - DAG(s) 'test_gcp_connection' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:45:37.177+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:45:37.176+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T10:45:37.202+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:45:37.201+0000] {dag.py:4138} INFO - Setting next_dagrun for test_gcp_connection to None, run_after=None
[2024-08-25T10:45:37.230+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.133 seconds
[2024-08-25T10:46:07.488+0000] {processor.py:186} INFO - Started process (PID=4980) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:46:07.489+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T10:46:07.493+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:46:07.493+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:46:07.528+0000] {processor.py:925} INFO - DAG(s) 'test_gcp_connection' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:46:07.556+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:46:07.555+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T10:46:07.580+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:46:07.580+0000] {dag.py:4138} INFO - Setting next_dagrun for test_gcp_connection to None, run_after=None
[2024-08-25T10:46:07.607+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.128 seconds
[2024-08-25T10:46:37.725+0000] {processor.py:186} INFO - Started process (PID=4992) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:46:37.727+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T10:46:37.730+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:46:37.729+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:46:37.773+0000] {processor.py:925} INFO - DAG(s) 'test_gcp_connection' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:46:37.803+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:46:37.802+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T10:46:37.823+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:46:37.822+0000] {dag.py:4138} INFO - Setting next_dagrun for test_gcp_connection to None, run_after=None
[2024-08-25T10:46:37.852+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.133 seconds
[2024-08-25T10:47:08.046+0000] {processor.py:186} INFO - Started process (PID=5004) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:47:08.047+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T10:47:08.051+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:47:08.051+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:47:08.081+0000] {processor.py:925} INFO - DAG(s) 'test_gcp_connection' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:47:08.111+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:47:08.110+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T10:47:08.132+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:47:08.131+0000] {dag.py:4138} INFO - Setting next_dagrun for test_gcp_connection to None, run_after=None
[2024-08-25T10:47:08.160+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.123 seconds
[2024-08-25T10:47:38.354+0000] {processor.py:186} INFO - Started process (PID=5016) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:47:38.356+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T10:47:38.361+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:47:38.361+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:47:38.387+0000] {processor.py:925} INFO - DAG(s) 'test_gcp_connection' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:47:38.413+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:47:38.413+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T10:47:38.439+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:47:38.439+0000] {dag.py:4138} INFO - Setting next_dagrun for test_gcp_connection to None, run_after=None
[2024-08-25T10:47:38.468+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.122 seconds
[2024-08-25T10:48:08.697+0000] {processor.py:186} INFO - Started process (PID=5028) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:48:08.700+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T10:48:08.705+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:48:08.704+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:48:08.736+0000] {processor.py:925} INFO - DAG(s) 'test_gcp_connection' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:48:08.769+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:48:08.768+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T10:48:08.796+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:48:08.796+0000] {dag.py:4138} INFO - Setting next_dagrun for test_gcp_connection to None, run_after=None
[2024-08-25T10:48:08.821+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.129 seconds
[2024-08-25T10:48:38.938+0000] {processor.py:186} INFO - Started process (PID=5044) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:48:38.941+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T10:48:38.946+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:48:38.945+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:48:38.979+0000] {processor.py:925} INFO - DAG(s) 'test_gcp_connection' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:48:39.006+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:48:39.005+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T10:48:39.037+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:48:39.037+0000] {dag.py:4138} INFO - Setting next_dagrun for test_gcp_connection to None, run_after=None
[2024-08-25T10:48:39.065+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.137 seconds
[2024-08-25T10:49:09.945+0000] {processor.py:186} INFO - Started process (PID=5056) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:49:09.947+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T10:49:09.950+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:49:09.949+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:49:10.000+0000] {processor.py:925} INFO - DAG(s) 'test_gcp_connection' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:49:10.024+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:49:10.024+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T10:49:10.046+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:49:10.045+0000] {dag.py:4138} INFO - Setting next_dagrun for test_gcp_connection to None, run_after=None
[2024-08-25T10:49:10.077+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.136 seconds
[2024-08-25T10:49:40.268+0000] {processor.py:186} INFO - Started process (PID=5068) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:49:40.270+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T10:49:40.275+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:49:40.274+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:49:40.319+0000] {processor.py:925} INFO - DAG(s) 'test_gcp_connection' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:49:40.371+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:49:40.370+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T10:49:40.421+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:49:40.421+0000] {dag.py:4138} INFO - Setting next_dagrun for test_gcp_connection to None, run_after=None
[2024-08-25T10:49:40.467+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.213 seconds
[2024-08-25T10:50:10.775+0000] {processor.py:186} INFO - Started process (PID=5081) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:50:10.777+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T10:50:10.779+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:50:10.779+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:50:10.823+0000] {processor.py:925} INFO - DAG(s) 'test_gcp_connection' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:50:10.848+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:50:10.847+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T10:50:10.875+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:50:10.875+0000] {dag.py:4138} INFO - Setting next_dagrun for test_gcp_connection to None, run_after=None
[2024-08-25T10:50:10.896+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.126 seconds
[2024-08-25T10:50:41.166+0000] {processor.py:186} INFO - Started process (PID=5093) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:50:41.167+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T10:50:41.171+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:50:41.171+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:50:41.201+0000] {processor.py:925} INFO - DAG(s) 'test_gcp_connection' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:50:41.241+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:50:41.241+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T10:50:41.287+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:50:41.286+0000] {dag.py:4138} INFO - Setting next_dagrun for test_gcp_connection to None, run_after=None
[2024-08-25T10:50:41.324+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.168 seconds
[2024-08-25T10:51:11.479+0000] {processor.py:186} INFO - Started process (PID=5105) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:51:11.481+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T10:51:11.485+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:51:11.484+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:51:11.526+0000] {processor.py:925} INFO - DAG(s) 'test_gcp_connection' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:51:11.579+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:51:11.578+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T10:51:11.622+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:51:11.621+0000] {dag.py:4138} INFO - Setting next_dagrun for test_gcp_connection to None, run_after=None
[2024-08-25T10:51:11.657+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.192 seconds
[2024-08-25T10:51:41.749+0000] {processor.py:186} INFO - Started process (PID=5117) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:51:41.750+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T10:51:41.755+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:51:41.754+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:51:41.785+0000] {processor.py:925} INFO - DAG(s) 'test_gcp_connection' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:51:41.826+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:51:41.825+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T10:51:41.864+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:51:41.863+0000] {dag.py:4138} INFO - Setting next_dagrun for test_gcp_connection to None, run_after=None
[2024-08-25T10:51:41.898+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.160 seconds
[2024-08-25T10:52:12.185+0000] {processor.py:186} INFO - Started process (PID=5129) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:52:12.186+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T10:52:12.191+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:52:12.191+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:52:12.232+0000] {processor.py:925} INFO - DAG(s) 'test_gcp_connection' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:52:12.259+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:52:12.258+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T10:52:12.278+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:52:12.277+0000] {dag.py:4138} INFO - Setting next_dagrun for test_gcp_connection to None, run_after=None
[2024-08-25T10:52:12.312+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.133 seconds
[2024-08-25T10:52:42.600+0000] {processor.py:186} INFO - Started process (PID=5141) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:52:42.603+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T10:52:42.606+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:52:42.605+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:52:42.638+0000] {processor.py:925} INFO - DAG(s) 'test_gcp_connection' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:52:42.671+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:52:42.670+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T10:52:42.706+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:52:42.706+0000] {dag.py:4138} INFO - Setting next_dagrun for test_gcp_connection to None, run_after=None
[2024-08-25T10:52:42.735+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.140 seconds
[2024-08-25T10:53:12.866+0000] {processor.py:186} INFO - Started process (PID=5153) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:53:12.868+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T10:53:12.871+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:53:12.870+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:53:12.918+0000] {processor.py:925} INFO - DAG(s) 'test_gcp_connection' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:53:12.949+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:53:12.949+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T10:53:12.979+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:53:12.978+0000] {dag.py:4138} INFO - Setting next_dagrun for test_gcp_connection to None, run_after=None
[2024-08-25T10:53:13.015+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.157 seconds
[2024-08-25T10:53:43.882+0000] {processor.py:186} INFO - Started process (PID=5165) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:53:43.884+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T10:53:43.887+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:53:43.886+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:53:43.907+0000] {processor.py:925} INFO - DAG(s) 'test_gcp_connection' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:53:43.956+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:53:43.950+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T10:53:44.008+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:53:44.008+0000] {dag.py:4138} INFO - Setting next_dagrun for test_gcp_connection to None, run_after=None
[2024-08-25T10:53:44.044+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.169 seconds
[2024-08-25T10:54:14.839+0000] {processor.py:186} INFO - Started process (PID=5177) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:54:14.840+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T10:54:14.844+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:54:14.844+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:54:14.888+0000] {processor.py:925} INFO - DAG(s) 'test_gcp_connection' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:54:14.914+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:54:14.914+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T10:54:14.935+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:54:14.934+0000] {dag.py:4138} INFO - Setting next_dagrun for test_gcp_connection to None, run_after=None
[2024-08-25T10:54:14.958+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.125 seconds
[2024-08-25T10:54:22.220+0000] {processor.py:186} INFO - Started process (PID=5180) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:54:22.221+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T10:54:22.223+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:54:22.223+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:54:22.244+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:54:22.344+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:54:22.344+0000] {override.py:1858} INFO - Created Permission View: can edit on DAG:gcs_connection_test_v2
[2024-08-25T10:54:22.353+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:54:22.353+0000] {override.py:1858} INFO - Created Permission View: can delete on DAG:gcs_connection_test_v2
[2024-08-25T10:54:22.359+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:54:22.359+0000] {override.py:1858} INFO - Created Permission View: can read on DAG:gcs_connection_test_v2
[2024-08-25T10:54:22.360+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:54:22.360+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T10:54:22.371+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:54:22.370+0000] {dag.py:3234} INFO - Creating ORM DAG for gcs_connection_test_v2
[2024-08-25T10:54:22.378+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:54:22.378+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to 2024-08-24 00:00:00+00:00, run_after=2024-08-24 00:00:00+00:00
[2024-08-25T10:54:22.403+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.189 seconds
[2024-08-25T10:54:52.597+0000] {processor.py:186} INFO - Started process (PID=5192) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:54:52.599+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T10:54:52.603+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:54:52.603+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:54:52.648+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:54:52.674+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:54:52.673+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T10:54:52.697+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:54:52.697+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T10:54:52.722+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.131 seconds
[2024-08-25T10:55:22.774+0000] {processor.py:186} INFO - Started process (PID=5204) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:55:22.775+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T10:55:22.779+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:55:22.778+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:55:22.815+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:55:22.846+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:55:22.846+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T10:55:22.866+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:55:22.866+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T10:55:22.887+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.120 seconds
[2024-08-25T10:55:53.004+0000] {processor.py:186} INFO - Started process (PID=5216) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:55:53.005+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T10:55:53.007+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:55:53.007+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:55:53.049+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:55:53.075+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:55:53.074+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T10:55:53.095+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:55:53.095+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T10:55:53.118+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.119 seconds
[2024-08-25T10:56:23.360+0000] {processor.py:186} INFO - Started process (PID=5228) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:56:23.363+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T10:56:23.366+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:56:23.365+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:56:23.391+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:56:23.426+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:56:23.426+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T10:56:23.454+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:56:23.454+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T10:56:23.477+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.129 seconds
[2024-08-25T10:56:54.561+0000] {processor.py:186} INFO - Started process (PID=5241) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:56:54.563+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T10:56:54.565+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:56:54.565+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:56:54.605+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:56:54.628+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:56:54.627+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T10:56:54.645+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:56:54.644+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T10:56:54.666+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.109 seconds
[2024-08-25T10:57:24.811+0000] {processor.py:186} INFO - Started process (PID=5252) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:57:24.813+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T10:57:24.816+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:57:24.816+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:57:24.865+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:57:24.889+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:57:24.889+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T10:57:24.909+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:57:24.909+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T10:57:24.938+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.136 seconds
[2024-08-25T10:57:55.955+0000] {processor.py:186} INFO - Started process (PID=5264) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:57:55.957+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T10:57:55.958+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:57:55.958+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:57:55.993+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:57:56.014+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:57:56.014+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T10:57:56.032+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:57:56.032+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T10:57:56.056+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.107 seconds
[2024-08-25T10:58:26.236+0000] {processor.py:186} INFO - Started process (PID=5276) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:58:26.238+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T10:58:26.239+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:58:26.239+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:58:26.288+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:58:26.326+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:58:26.326+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T10:58:26.357+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:58:26.357+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T10:58:26.384+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.156 seconds
[2024-08-25T10:58:56.530+0000] {processor.py:186} INFO - Started process (PID=5288) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:58:56.532+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T10:58:56.534+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:58:56.533+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:58:56.562+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:58:56.586+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:58:56.585+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T10:58:56.615+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:58:56.614+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T10:58:56.641+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.115 seconds
[2024-08-25T10:59:26.787+0000] {processor.py:186} INFO - Started process (PID=5300) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:59:26.789+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T10:59:26.790+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:59:26.790+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:59:26.817+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:59:26.845+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:59:26.845+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T10:59:26.871+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:59:26.871+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T10:59:26.888+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.105 seconds
[2024-08-25T10:59:56.991+0000] {processor.py:186} INFO - Started process (PID=5312) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:59:56.992+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T10:59:56.993+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:59:56.993+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:59:57.008+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T10:59:57.029+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:59:57.029+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T10:59:57.065+0000] {logging_mixin.py:190} INFO - [2024-08-25T10:59:57.064+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T10:59:57.095+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.108 seconds
[2024-08-25T11:00:27.226+0000] {processor.py:186} INFO - Started process (PID=5323) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:00:27.227+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:00:27.229+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:00:27.229+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:00:27.267+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:00:27.289+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:00:27.288+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:00:27.306+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:00:27.306+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:00:27.325+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.104 seconds
[2024-08-25T11:00:57.614+0000] {processor.py:186} INFO - Started process (PID=5335) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:00:57.615+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:00:57.617+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:00:57.617+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:00:57.649+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:00:57.670+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:00:57.669+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:00:57.687+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:00:57.687+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:00:57.717+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.109 seconds
[2024-08-25T11:01:27.925+0000] {processor.py:186} INFO - Started process (PID=5347) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:01:27.927+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:01:27.928+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:01:27.928+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:01:27.961+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:01:27.986+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:01:27.985+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:01:28.007+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:01:28.006+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:01:28.027+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.107 seconds
[2024-08-25T11:01:58.145+0000] {processor.py:186} INFO - Started process (PID=5359) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:01:58.146+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:01:58.148+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:01:58.148+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:01:58.183+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:01:58.212+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:01:58.211+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:01:58.234+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:01:58.234+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:01:58.267+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.129 seconds
[2024-08-25T11:02:28.629+0000] {processor.py:186} INFO - Started process (PID=5371) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:02:28.632+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:02:28.635+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:02:28.634+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:02:28.695+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:02:28.730+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:02:28.730+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:02:28.758+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:02:28.758+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:02:28.795+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.174 seconds
[2024-08-25T11:02:58.917+0000] {processor.py:186} INFO - Started process (PID=5382) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:02:58.919+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:02:58.921+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:02:58.920+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:02:58.972+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:02:59.006+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:02:59.006+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:02:59.044+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:02:59.043+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:02:59.073+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.163 seconds
[2024-08-25T11:03:29.216+0000] {processor.py:186} INFO - Started process (PID=5394) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:03:29.219+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:03:29.226+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:03:29.223+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:03:29.295+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:03:29.350+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:03:29.350+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:03:29.383+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:03:29.383+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:03:29.420+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.215 seconds
[2024-08-25T11:03:59.508+0000] {processor.py:186} INFO - Started process (PID=5406) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:03:59.510+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:03:59.514+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:03:59.513+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:03:59.537+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:03:59.571+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:03:59.571+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:03:59.612+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:03:59.609+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:03:59.635+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.136 seconds
[2024-08-25T11:04:29.810+0000] {processor.py:186} INFO - Started process (PID=5424) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:04:29.811+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:04:29.815+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:04:29.814+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:04:29.835+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:04:29.862+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:04:29.862+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:04:29.883+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:04:29.883+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:04:29.900+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.099 seconds
[2024-08-25T11:05:00.753+0000] {processor.py:186} INFO - Started process (PID=5435) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:05:00.755+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:05:00.757+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:05:00.757+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:05:00.800+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:05:00.823+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:05:00.823+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:05:00.840+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:05:00.840+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:05:00.862+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.113 seconds
[2024-08-25T11:05:31.096+0000] {processor.py:186} INFO - Started process (PID=5447) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:05:31.098+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:05:31.100+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:05:31.099+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:05:31.144+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:05:31.166+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:05:31.165+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:05:31.182+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:05:31.182+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:05:31.203+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.113 seconds
[2024-08-25T11:06:01.366+0000] {processor.py:186} INFO - Started process (PID=5459) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:06:01.368+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:06:01.369+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:06:01.369+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:06:01.400+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:06:01.423+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:06:01.422+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:06:01.442+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:06:01.441+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:06:01.459+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.100 seconds
[2024-08-25T11:06:31.879+0000] {processor.py:186} INFO - Started process (PID=5471) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:06:31.880+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:06:31.882+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:06:31.882+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:06:31.903+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:06:31.933+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:06:31.933+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:06:31.963+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:06:31.963+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:06:31.984+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.116 seconds
[2024-08-25T11:07:02.956+0000] {processor.py:186} INFO - Started process (PID=5483) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:07:02.958+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:07:02.962+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:07:02.962+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:07:03.017+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:07:03.056+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:07:03.056+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:07:03.090+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:07:03.089+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:07:03.120+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.174 seconds
[2024-08-25T11:07:33.461+0000] {processor.py:186} INFO - Started process (PID=5495) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:07:33.463+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:07:33.465+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:07:33.464+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:07:33.493+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:07:33.528+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:07:33.528+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:07:33.559+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:07:33.559+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:07:33.587+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.134 seconds
[2024-08-25T11:08:03.682+0000] {processor.py:186} INFO - Started process (PID=5507) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:08:03.683+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:08:03.686+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:08:03.685+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:08:03.719+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:08:03.740+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:08:03.740+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:08:03.760+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:08:03.760+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:08:03.783+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.107 seconds
[2024-08-25T11:08:33.919+0000] {processor.py:186} INFO - Started process (PID=5519) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:08:33.920+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:08:33.922+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:08:33.922+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:08:33.971+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:08:33.995+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:08:33.994+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:08:34.013+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:08:34.013+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:08:34.036+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.122 seconds
[2024-08-25T11:09:04.203+0000] {processor.py:186} INFO - Started process (PID=5531) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:09:04.205+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:09:04.209+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:09:04.208+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:09:04.252+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:09:04.281+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:09:04.281+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:09:04.313+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:09:04.312+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:09:04.346+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.148 seconds
[2024-08-25T11:09:34.533+0000] {processor.py:186} INFO - Started process (PID=5543) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:09:34.535+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:09:34.537+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:09:34.537+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:09:34.577+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:09:34.607+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:09:34.607+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:09:34.638+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:09:34.638+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:09:34.666+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.141 seconds
[2024-08-25T11:10:04.858+0000] {processor.py:186} INFO - Started process (PID=5555) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:10:04.860+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:10:04.862+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:10:04.861+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:10:04.910+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:10:04.947+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:10:04.946+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:10:04.985+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:10:04.985+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:10:05.018+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.166 seconds
[2024-08-25T11:10:35.140+0000] {processor.py:186} INFO - Started process (PID=5567) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:10:35.142+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:10:35.144+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:10:35.144+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:10:35.196+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:10:35.228+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:10:35.228+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:10:35.256+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:10:35.256+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:10:35.283+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.148 seconds
[2024-08-25T11:11:06.324+0000] {processor.py:186} INFO - Started process (PID=5579) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:11:06.326+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:11:06.327+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:11:06.327+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:11:06.379+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:11:06.401+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:11:06.401+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:11:06.424+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:11:06.423+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:11:06.444+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.124 seconds
[2024-08-25T11:11:36.665+0000] {processor.py:186} INFO - Started process (PID=5591) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:11:36.667+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:11:36.669+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:11:36.669+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:11:36.715+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:11:36.745+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:11:36.745+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:11:36.771+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:11:36.771+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:11:36.799+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.139 seconds
[2024-08-25T11:12:06.920+0000] {processor.py:186} INFO - Started process (PID=5604) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:12:06.921+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:12:06.923+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:12:06.922+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:12:06.942+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:12:06.975+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:12:06.975+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:12:07.010+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:12:07.010+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:12:07.037+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.125 seconds
[2024-08-25T11:12:37.293+0000] {processor.py:186} INFO - Started process (PID=5616) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:12:37.295+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:12:37.297+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:12:37.296+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:12:37.343+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:12:37.371+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:12:37.370+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:12:37.398+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:12:37.398+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:12:37.423+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.138 seconds
[2024-08-25T11:13:07.556+0000] {processor.py:186} INFO - Started process (PID=5629) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:13:07.557+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:13:07.559+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:13:07.559+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:13:07.573+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:13:07.597+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:13:07.597+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:13:07.624+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:13:07.623+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:13:07.646+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.094 seconds
[2024-08-25T11:13:37.802+0000] {processor.py:186} INFO - Started process (PID=5641) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:13:37.803+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:13:37.805+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:13:37.804+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:13:37.826+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:13:37.854+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:13:37.854+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:13:37.874+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:13:37.874+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:13:37.892+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.095 seconds
[2024-08-25T11:14:08.876+0000] {processor.py:186} INFO - Started process (PID=5653) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:14:08.878+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:14:08.880+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:14:08.879+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:14:08.917+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:14:08.946+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:14:08.946+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:14:08.963+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:14:08.962+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:14:08.983+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.112 seconds
[2024-08-25T11:14:39.168+0000] {processor.py:186} INFO - Started process (PID=5665) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:14:39.169+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:14:39.171+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:14:39.170+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:14:39.210+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:14:39.233+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:14:39.233+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:14:39.251+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:14:39.251+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:14:39.272+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.108 seconds
[2024-08-25T11:15:09.598+0000] {processor.py:186} INFO - Started process (PID=5677) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:15:09.600+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:15:09.602+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:15:09.601+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:15:09.635+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:15:09.656+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:15:09.656+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:15:09.675+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:15:09.675+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:15:09.698+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.106 seconds
[2024-08-25T11:15:39.793+0000] {processor.py:186} INFO - Started process (PID=5689) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:15:39.794+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:15:39.797+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:15:39.796+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:15:39.845+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:15:39.867+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:15:39.867+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:15:39.885+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:15:39.885+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:15:39.907+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.120 seconds
[2024-08-25T11:16:10.129+0000] {processor.py:186} INFO - Started process (PID=5701) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:16:10.131+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:16:10.133+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:16:10.133+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:16:10.178+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:16:10.212+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:16:10.211+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:16:10.233+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:16:10.233+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:16:10.251+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.127 seconds
[2024-08-25T11:16:40.357+0000] {processor.py:186} INFO - Started process (PID=5713) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:16:40.358+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:16:40.360+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:16:40.359+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:16:40.388+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:16:40.427+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:16:40.426+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:16:40.451+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:16:40.450+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:16:40.470+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.121 seconds
[2024-08-25T11:17:10.582+0000] {processor.py:186} INFO - Started process (PID=5725) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:17:10.583+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:17:10.586+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:17:10.585+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:17:10.616+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:17:10.645+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:17:10.645+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:17:10.667+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:17:10.667+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:17:10.688+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.115 seconds
[2024-08-25T11:17:40.956+0000] {processor.py:186} INFO - Started process (PID=5737) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:17:40.958+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:17:40.959+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:17:40.959+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:17:41.001+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:17:41.030+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:17:41.030+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:17:41.048+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:17:41.048+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:17:41.077+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.125 seconds
[2024-08-25T11:18:11.273+0000] {processor.py:186} INFO - Started process (PID=5749) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:18:11.274+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:18:11.276+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:18:11.276+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:18:11.322+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:18:11.344+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:18:11.344+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:18:11.361+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:18:11.361+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:18:11.383+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.115 seconds
[2024-08-25T11:18:41.479+0000] {processor.py:186} INFO - Started process (PID=5761) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:18:41.481+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:18:41.482+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:18:41.482+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:18:41.520+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:18:41.545+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:18:41.545+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:18:41.567+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:18:41.567+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:18:41.590+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.116 seconds
[2024-08-25T11:19:11.889+0000] {processor.py:186} INFO - Started process (PID=5773) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:19:11.891+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:19:11.893+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:19:11.892+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:19:11.927+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:19:11.948+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:19:11.948+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:19:11.965+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:19:11.965+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:19:11.986+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.100 seconds
[2024-08-25T11:19:42.155+0000] {processor.py:186} INFO - Started process (PID=5785) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:19:42.157+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:19:42.159+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:19:42.159+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:19:42.205+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:19:42.227+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:19:42.227+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:19:42.247+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:19:42.247+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:19:42.269+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.118 seconds
[2024-08-25T11:20:12.369+0000] {processor.py:186} INFO - Started process (PID=5797) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:20:12.370+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:20:12.371+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:20:12.371+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:20:12.412+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:20:12.433+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:20:12.433+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:20:12.450+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:20:12.450+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:20:12.481+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.116 seconds
[2024-08-25T11:20:42.654+0000] {processor.py:186} INFO - Started process (PID=5809) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:20:42.656+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:20:42.658+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:20:42.658+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:20:42.709+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:20:42.731+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:20:42.731+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:20:42.751+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:20:42.751+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:20:42.779+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.129 seconds
[2024-08-25T11:21:12.954+0000] {processor.py:186} INFO - Started process (PID=5821) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:21:12.955+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:21:12.956+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:21:12.956+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:21:12.977+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:21:13.000+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:21:12.999+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:21:13.020+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:21:13.020+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:21:13.039+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.089 seconds
[2024-08-25T11:21:43.223+0000] {processor.py:186} INFO - Started process (PID=5833) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:21:43.225+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:21:43.226+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:21:43.226+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:21:43.256+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:21:43.278+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:21:43.278+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:21:43.297+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:21:43.297+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:21:43.319+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.101 seconds
[2024-08-25T11:22:13.534+0000] {processor.py:186} INFO - Started process (PID=5845) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:22:13.535+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:22:13.537+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:22:13.537+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:22:13.555+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:22:13.575+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:22:13.575+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:22:13.594+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:22:13.594+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:22:13.623+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.096 seconds
[2024-08-25T11:22:43.866+0000] {processor.py:186} INFO - Started process (PID=5857) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:22:43.868+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:22:43.869+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:22:43.869+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:22:43.910+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:22:43.937+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:22:43.937+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:22:43.956+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:22:43.955+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:22:43.978+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.116 seconds
[2024-08-25T11:23:14.250+0000] {processor.py:186} INFO - Started process (PID=5869) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:23:14.251+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:23:14.253+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:23:14.253+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:23:14.297+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:23:14.320+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:23:14.320+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:23:14.340+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:23:14.339+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:23:14.362+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.117 seconds
[2024-08-25T11:23:44.541+0000] {processor.py:186} INFO - Started process (PID=5881) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:23:44.542+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:23:44.544+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:23:44.544+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:23:44.583+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:23:44.607+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:23:44.607+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:23:44.624+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:23:44.624+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:23:44.644+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.109 seconds
[2024-08-25T11:24:14.830+0000] {processor.py:186} INFO - Started process (PID=5893) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:24:14.831+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:24:14.833+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:24:14.833+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:24:14.876+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:24:14.898+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:24:14.897+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:24:14.915+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:24:14.915+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:24:14.936+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.110 seconds
[2024-08-25T11:24:45.048+0000] {processor.py:186} INFO - Started process (PID=5905) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:24:45.050+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:24:45.052+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:24:45.052+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:24:45.099+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:24:45.128+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:24:45.128+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:24:45.147+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:24:45.147+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:24:45.168+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.124 seconds
[2024-08-25T11:25:15.272+0000] {processor.py:186} INFO - Started process (PID=5916) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:25:15.274+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:25:15.276+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:25:15.276+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:25:15.322+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:25:15.348+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:25:15.348+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:25:15.367+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:25:15.367+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:25:15.386+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.119 seconds
[2024-08-25T11:25:45.542+0000] {processor.py:186} INFO - Started process (PID=5928) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:25:45.543+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:25:45.545+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:25:45.545+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:25:45.589+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:25:45.611+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:25:45.611+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:25:45.629+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:25:45.629+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:25:45.658+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.122 seconds
[2024-08-25T11:26:15.929+0000] {processor.py:186} INFO - Started process (PID=5940) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:26:15.930+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:26:15.932+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:26:15.932+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:26:15.978+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:26:15.999+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:26:15.999+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:26:16.018+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:26:16.018+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:26:16.039+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.114 seconds
[2024-08-25T11:26:46.155+0000] {processor.py:186} INFO - Started process (PID=5952) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:26:46.156+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:26:46.158+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:26:46.157+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:26:46.190+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:26:46.213+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:26:46.212+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:26:46.230+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:26:46.230+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:26:46.252+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.105 seconds
[2024-08-25T11:27:16.411+0000] {processor.py:186} INFO - Started process (PID=5964) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:27:16.412+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:27:16.414+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:27:16.414+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:27:16.463+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:27:16.486+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:27:16.486+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:27:16.504+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:27:16.504+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:27:16.534+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.128 seconds
[2024-08-25T11:27:47.581+0000] {processor.py:186} INFO - Started process (PID=5976) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:27:47.583+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:27:47.584+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:27:47.584+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:27:47.625+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:27:47.648+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:27:47.648+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:27:47.665+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:27:47.665+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:27:47.692+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.115 seconds
[2024-08-25T11:28:17.930+0000] {processor.py:186} INFO - Started process (PID=5988) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:28:17.932+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:28:17.935+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:28:17.934+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:28:17.978+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:28:18.001+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:28:18.000+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:28:18.018+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:28:18.018+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:28:18.047+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.125 seconds
[2024-08-25T11:28:48.173+0000] {processor.py:186} INFO - Started process (PID=6000) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:28:48.174+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:28:48.176+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:28:48.176+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:28:48.210+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:28:48.234+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:28:48.234+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:28:48.261+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:28:48.261+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:28:48.287+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.122 seconds
[2024-08-25T11:29:18.404+0000] {processor.py:186} INFO - Started process (PID=6011) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:29:18.405+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:29:18.407+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:29:18.406+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:29:18.442+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:29:18.472+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:29:18.472+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:29:18.498+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:29:18.497+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:29:18.515+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.116 seconds
[2024-08-25T11:29:48.683+0000] {processor.py:186} INFO - Started process (PID=6023) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:29:48.684+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:29:48.686+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:29:48.686+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:29:48.701+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:29:48.723+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:29:48.723+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:29:48.743+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:29:48.742+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:29:48.766+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.091 seconds
[2024-08-25T11:30:18.981+0000] {processor.py:186} INFO - Started process (PID=6036) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:30:18.983+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:30:18.985+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:30:18.984+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:30:19.016+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:30:19.039+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:30:19.039+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:30:19.057+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:30:19.057+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:30:19.078+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.103 seconds
[2024-08-25T11:30:49.352+0000] {processor.py:186} INFO - Started process (PID=6048) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:30:49.353+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:30:49.355+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:30:49.354+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:30:49.391+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:30:49.414+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:30:49.414+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:30:49.437+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:30:49.437+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:30:49.465+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.120 seconds
[2024-08-25T11:31:19.784+0000] {processor.py:186} INFO - Started process (PID=6059) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:31:19.785+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:31:19.786+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:31:19.786+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:31:19.821+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:31:19.842+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:31:19.841+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:31:19.859+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:31:19.859+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:31:19.887+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.107 seconds
[2024-08-25T11:31:49.992+0000] {processor.py:186} INFO - Started process (PID=6070) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:31:49.994+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:31:49.995+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:31:49.995+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:31:50.039+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:31:50.069+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:31:50.069+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:31:50.090+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:31:50.089+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:31:50.111+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.123 seconds
[2024-08-25T11:32:20.334+0000] {processor.py:186} INFO - Started process (PID=6082) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:32:20.336+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:32:20.338+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:32:20.338+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:32:20.377+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:32:20.404+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:32:20.404+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:32:20.423+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:32:20.423+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:32:20.444+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.115 seconds
[2024-08-25T11:32:50.687+0000] {processor.py:186} INFO - Started process (PID=6095) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:32:50.689+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:32:50.690+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:32:50.690+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:32:50.736+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:32:50.758+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:32:50.757+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:32:50.775+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:32:50.775+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:32:50.803+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.120 seconds
[2024-08-25T11:33:20.992+0000] {processor.py:186} INFO - Started process (PID=6107) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:33:20.993+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:33:20.994+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:33:20.994+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:33:21.026+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:33:21.059+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:33:21.059+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:33:21.083+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:33:21.083+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:33:21.109+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.122 seconds
[2024-08-25T11:33:51.298+0000] {processor.py:186} INFO - Started process (PID=6119) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:33:51.300+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:33:51.301+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:33:51.301+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:33:51.317+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:33:51.348+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:33:51.348+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:33:51.366+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:33:51.366+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:33:51.383+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.093 seconds
[2024-08-25T11:34:21.590+0000] {processor.py:186} INFO - Started process (PID=6131) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:34:21.592+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:34:21.593+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:34:21.593+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:34:21.646+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:34:21.669+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:34:21.669+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:34:21.687+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:34:21.687+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:34:21.718+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.133 seconds
[2024-08-25T11:34:51.901+0000] {processor.py:186} INFO - Started process (PID=6143) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:34:51.902+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:34:51.904+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:34:51.903+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:34:51.946+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:34:51.968+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:34:51.968+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:34:51.988+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:34:51.988+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:34:52.009+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.113 seconds
[2024-08-25T11:35:22.173+0000] {processor.py:186} INFO - Started process (PID=6155) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:35:22.175+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:35:22.177+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:35:22.176+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:35:22.221+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:35:22.243+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:35:22.243+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:35:22.260+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:35:22.260+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:35:22.286+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.118 seconds
[2024-08-25T11:35:52.390+0000] {processor.py:186} INFO - Started process (PID=6167) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:35:52.391+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:35:52.393+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:35:52.392+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:35:52.438+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:35:52.463+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:35:52.463+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:35:52.480+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:35:52.480+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:35:52.502+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.117 seconds
[2024-08-25T11:36:22.593+0000] {processor.py:186} INFO - Started process (PID=6179) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:36:22.594+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:36:22.596+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:36:22.596+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:36:22.631+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:36:22.661+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:36:22.661+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:36:22.679+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:36:22.679+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:36:22.700+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.112 seconds
[2024-08-25T11:36:52.875+0000] {processor.py:186} INFO - Started process (PID=6192) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:36:52.876+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:36:52.878+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:36:52.878+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:36:52.919+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:36:52.942+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:36:52.942+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:36:52.960+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:36:52.960+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:36:52.989+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.120 seconds
[2024-08-25T11:37:23.263+0000] {processor.py:186} INFO - Started process (PID=6204) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:37:23.265+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:37:23.267+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:37:23.266+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:37:23.304+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:37:23.368+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:37:23.368+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:37:23.400+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:37:23.399+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:37:23.420+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.164 seconds
[2024-08-25T11:37:53.694+0000] {processor.py:186} INFO - Started process (PID=6216) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:37:53.695+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:37:53.697+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:37:53.696+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:37:53.707+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:37:53.725+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:37:53.725+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:37:53.747+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:37:53.747+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:37:53.769+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.079 seconds
[2024-08-25T11:38:23.955+0000] {processor.py:186} INFO - Started process (PID=6228) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:38:23.956+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:38:23.958+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:38:23.958+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:38:24.005+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:38:24.026+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:38:24.026+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:38:24.044+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:38:24.044+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:38:24.064+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.114 seconds
[2024-08-25T11:38:54.313+0000] {processor.py:186} INFO - Started process (PID=6240) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:38:54.315+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:38:54.317+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:38:54.316+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:38:54.353+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:38:54.374+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:38:54.374+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:38:54.392+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:38:54.392+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:38:54.413+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.108 seconds
[2024-08-25T11:39:24.527+0000] {processor.py:186} INFO - Started process (PID=6252) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:39:24.528+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:39:24.529+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:39:24.529+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:39:24.567+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:39:24.589+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:39:24.589+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:39:24.606+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:39:24.606+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:39:24.627+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.106 seconds
[2024-08-25T11:39:54.906+0000] {processor.py:186} INFO - Started process (PID=6264) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:39:54.907+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:39:54.909+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:39:54.908+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:39:54.938+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:39:54.965+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:39:54.965+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:39:54.986+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:39:54.986+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:39:55.006+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.106 seconds
[2024-08-25T11:40:25.159+0000] {processor.py:186} INFO - Started process (PID=6276) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:40:25.162+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:40:25.164+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:40:25.164+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:40:25.208+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:40:25.246+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:40:25.245+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:40:25.279+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:40:25.278+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:40:25.308+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.155 seconds
[2024-08-25T11:40:55.615+0000] {processor.py:186} INFO - Started process (PID=6288) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:40:55.616+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:40:55.619+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:40:55.619+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:40:55.670+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:40:55.700+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:40:55.700+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:40:55.725+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:40:55.724+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:40:55.754+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.145 seconds
[2024-08-25T11:41:25.998+0000] {processor.py:186} INFO - Started process (PID=6300) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:41:25.999+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:41:26.001+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:41:26.001+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:41:26.037+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:41:26.064+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:41:26.064+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:41:26.084+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:41:26.084+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:41:26.113+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.119 seconds
[2024-08-25T11:41:56.274+0000] {processor.py:186} INFO - Started process (PID=6312) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:41:56.275+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:41:56.277+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:41:56.277+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:41:56.310+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:41:56.343+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:41:56.343+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:41:56.372+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:41:56.372+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:41:56.405+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.137 seconds
[2024-08-25T11:42:26.700+0000] {processor.py:186} INFO - Started process (PID=6324) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:42:26.704+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:42:26.706+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:42:26.706+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:42:26.733+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:42:26.772+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:42:26.772+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:42:26.832+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:42:26.832+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:42:26.872+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.181 seconds
[2024-08-25T11:42:57.226+0000] {processor.py:186} INFO - Started process (PID=6337) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:42:57.228+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:42:57.230+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:42:57.229+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:42:57.255+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:42:57.296+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:42:57.296+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:42:57.330+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:42:57.330+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:42:57.352+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.141 seconds
[2024-08-25T11:43:27.440+0000] {processor.py:186} INFO - Started process (PID=6349) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:43:27.441+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:43:27.443+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:43:27.443+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:43:27.475+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:43:27.506+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:43:27.506+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:43:27.528+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:43:27.527+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:43:27.553+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.119 seconds
[2024-08-25T11:43:58.169+0000] {processor.py:186} INFO - Started process (PID=6361) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:43:58.171+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:43:58.173+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:43:58.173+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:43:58.211+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:43:58.249+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:43:58.249+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:43:58.282+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:43:58.282+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:43:58.310+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.150 seconds
[2024-08-25T11:44:29.058+0000] {processor.py:186} INFO - Started process (PID=6373) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:44:29.059+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:44:29.060+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:44:29.060+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:44:29.076+0000] {processor.py:925} INFO - DAG(s) 'gcs_connection_test_v2' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:44:29.098+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:44:29.098+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T11:44:29.120+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:44:29.119+0000] {dag.py:4138} INFO - Setting next_dagrun for gcs_connection_test_v2 to None, run_after=None
[2024-08-25T11:44:29.143+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.090 seconds
[2024-08-25T11:44:57.194+0000] {processor.py:186} INFO - Started process (PID=6385) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:44:57.196+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:44:57.197+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:44:57.197+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:44:57.280+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:44:57.264+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 3, in <module>
    from airflow.providers.google.cloud.operators.gcs import GCSListBucketsOperator
ImportError: cannot import name 'GCSListBucketsOperator' from 'airflow.providers.google.cloud.operators.gcs' (/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/google/cloud/operators/gcs.py)
[2024-08-25T11:44:57.285+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:44:57.327+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.140 seconds
[2024-08-25T11:45:27.515+0000] {processor.py:186} INFO - Started process (PID=6397) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:45:27.516+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:45:27.518+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:45:27.517+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:45:27.562+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:45:27.554+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 3, in <module>
    from airflow.providers.google.cloud.operators.gcs import GCSListBucketsOperator
ImportError: cannot import name 'GCSListBucketsOperator' from 'airflow.providers.google.cloud.operators.gcs' (/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/google/cloud/operators/gcs.py)
[2024-08-25T11:45:27.565+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:45:27.592+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.081 seconds
[2024-08-25T11:45:57.810+0000] {processor.py:186} INFO - Started process (PID=6409) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:45:57.812+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:45:57.815+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:45:57.814+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:45:57.853+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:45:57.845+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 3, in <module>
    from airflow.providers.google.cloud.operators.gcs import GCSListBucketsOperator
ImportError: cannot import name 'GCSListBucketsOperator' from 'airflow.providers.google.cloud.operators.gcs' (/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/google/cloud/operators/gcs.py)
[2024-08-25T11:45:57.856+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:45:57.878+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.076 seconds
[2024-08-25T11:46:28.072+0000] {processor.py:186} INFO - Started process (PID=6421) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:46:28.074+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:46:28.076+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:46:28.076+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:46:28.094+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:46:28.091+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 3, in <module>
    from airflow.providers.google.cloud.operators.gcs import GCSListBucketsOperator
ImportError: cannot import name 'GCSListBucketsOperator' from 'airflow.providers.google.cloud.operators.gcs' (/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/google/cloud/operators/gcs.py)
[2024-08-25T11:46:28.095+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:46:28.113+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.048 seconds
[2024-08-25T11:46:58.295+0000] {processor.py:186} INFO - Started process (PID=6433) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:46:58.297+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:46:58.299+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:46:58.299+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:46:58.343+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:46:58.333+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 3, in <module>
    from airflow.providers.google.cloud.operators.gcs import GCSListBucketsOperator
ImportError: cannot import name 'GCSListBucketsOperator' from 'airflow.providers.google.cloud.operators.gcs' (/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/google/cloud/operators/gcs.py)
[2024-08-25T11:46:58.347+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:46:58.375+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.104 seconds
[2024-08-25T11:47:28.535+0000] {processor.py:186} INFO - Started process (PID=6445) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:47:28.536+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:47:28.539+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:47:28.539+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:47:28.584+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:47:28.574+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 3, in <module>
    from airflow.providers.google.cloud.operators.gcs import GCSListBucketsOperator
ImportError: cannot import name 'GCSListBucketsOperator' from 'airflow.providers.google.cloud.operators.gcs' (/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/google/cloud/operators/gcs.py)
[2024-08-25T11:47:28.588+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:47:28.612+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.081 seconds
[2024-08-25T11:47:58.762+0000] {processor.py:186} INFO - Started process (PID=6457) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:47:58.764+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:47:58.766+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:47:58.766+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:47:58.816+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:47:58.806+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 3, in <module>
    from airflow.providers.google.cloud.operators.gcs import GCSListBucketsOperator
ImportError: cannot import name 'GCSListBucketsOperator' from 'airflow.providers.google.cloud.operators.gcs' (/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/google/cloud/operators/gcs.py)
[2024-08-25T11:47:58.820+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:47:58.845+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.089 seconds
[2024-08-25T11:48:28.984+0000] {processor.py:186} INFO - Started process (PID=6469) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:48:28.986+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:48:28.993+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:48:28.992+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:48:29.025+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:48:29.018+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 3, in <module>
    from airflow.providers.google.cloud.operators.gcs import GCSListBucketsOperator
ImportError: cannot import name 'GCSListBucketsOperator' from 'airflow.providers.google.cloud.operators.gcs' (/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/google/cloud/operators/gcs.py)
[2024-08-25T11:48:29.029+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:48:29.066+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.094 seconds
[2024-08-25T11:48:59.258+0000] {processor.py:186} INFO - Started process (PID=6481) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:48:59.260+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:48:59.262+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:48:59.261+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:48:59.286+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:48:59.281+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 3, in <module>
    from airflow.providers.google.cloud.operators.gcs import GCSListBucketsOperator
ImportError: cannot import name 'GCSListBucketsOperator' from 'airflow.providers.google.cloud.operators.gcs' (/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/google/cloud/operators/gcs.py)
[2024-08-25T11:48:59.288+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:48:59.319+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.068 seconds
[2024-08-25T11:49:29.501+0000] {processor.py:186} INFO - Started process (PID=6493) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:49:29.502+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:49:29.504+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:49:29.504+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:49:29.534+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:49:29.528+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 3, in <module>
    from airflow.providers.google.cloud.operators.gcs import GCSListBucketsOperator
ImportError: cannot import name 'GCSListBucketsOperator' from 'airflow.providers.google.cloud.operators.gcs' (/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/google/cloud/operators/gcs.py)
[2024-08-25T11:49:29.537+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:49:29.564+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.069 seconds
[2024-08-25T11:49:59.839+0000] {processor.py:186} INFO - Started process (PID=6505) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:49:59.840+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:49:59.842+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:49:59.842+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:49:59.864+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:49:59.856+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 3, in <module>
    from airflow.providers.google.cloud.operators.gcs import GCSListBucketsOperator
ImportError: cannot import name 'GCSListBucketsOperator' from 'airflow.providers.google.cloud.operators.gcs' (/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/google/cloud/operators/gcs.py)
[2024-08-25T11:49:59.867+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:49:59.892+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.064 seconds
[2024-08-25T11:50:30.120+0000] {processor.py:186} INFO - Started process (PID=6517) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:50:30.121+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:50:30.123+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:50:30.123+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:50:30.172+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:50:30.162+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 3, in <module>
    from airflow.providers.google.cloud.operators.gcs import GCSListBucketsOperator
ImportError: cannot import name 'GCSListBucketsOperator' from 'airflow.providers.google.cloud.operators.gcs' (/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/google/cloud/operators/gcs.py)
[2024-08-25T11:50:30.177+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:50:30.198+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.084 seconds
[2024-08-25T11:51:00.420+0000] {processor.py:186} INFO - Started process (PID=6529) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:51:00.421+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:51:00.423+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:51:00.423+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:51:00.458+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:51:00.449+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 3, in <module>
    from airflow.providers.google.cloud.operators.gcs import GCSListBucketsOperator
ImportError: cannot import name 'GCSListBucketsOperator' from 'airflow.providers.google.cloud.operators.gcs' (/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/google/cloud/operators/gcs.py)
[2024-08-25T11:51:00.461+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:51:00.482+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.067 seconds
[2024-08-25T11:51:30.764+0000] {processor.py:186} INFO - Started process (PID=6541) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:51:30.766+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:51:30.768+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:51:30.768+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:51:30.819+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:51:30.804+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 3, in <module>
    from airflow.providers.google.cloud.operators.gcs import GCSListBucketsOperator
ImportError: cannot import name 'GCSListBucketsOperator' from 'airflow.providers.google.cloud.operators.gcs' (/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/google/cloud/operators/gcs.py)
[2024-08-25T11:51:30.823+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:51:30.853+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.096 seconds
[2024-08-25T11:52:01.197+0000] {processor.py:186} INFO - Started process (PID=6553) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:52:01.198+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:52:01.200+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:52:01.200+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:52:01.257+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:52:01.244+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 3, in <module>
    from airflow.providers.google.cloud.operators.gcs import GCSListBucketsOperator
ImportError: cannot import name 'GCSListBucketsOperator' from 'airflow.providers.google.cloud.operators.gcs' (/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/google/cloud/operators/gcs.py)
[2024-08-25T11:52:01.260+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:52:01.291+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.099 seconds
[2024-08-25T11:52:31.454+0000] {processor.py:186} INFO - Started process (PID=6565) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:52:31.457+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:52:31.460+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:52:31.459+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:52:31.502+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:52:31.486+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 3, in <module>
    from airflow.providers.google.cloud.operators.gcs import GCSListBucketsOperator
ImportError: cannot import name 'GCSListBucketsOperator' from 'airflow.providers.google.cloud.operators.gcs' (/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/google/cloud/operators/gcs.py)
[2024-08-25T11:52:31.506+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:52:31.537+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.093 seconds
[2024-08-25T11:53:01.807+0000] {processor.py:186} INFO - Started process (PID=6577) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:53:01.808+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:53:01.810+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:53:01.809+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:53:01.840+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:53:01.831+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 3, in <module>
    from airflow.providers.google.cloud.operators.gcs import GCSListBucketsOperator
ImportError: cannot import name 'GCSListBucketsOperator' from 'airflow.providers.google.cloud.operators.gcs' (/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/google/cloud/operators/gcs.py)
[2024-08-25T11:53:01.842+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:53:01.872+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.073 seconds
[2024-08-25T11:53:32.307+0000] {processor.py:186} INFO - Started process (PID=6589) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:53:32.308+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:53:32.310+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:53:32.310+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:53:32.341+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:53:32.334+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 3, in <module>
    from airflow.providers.google.cloud.operators.gcs import GCSListBucketsOperator
ImportError: cannot import name 'GCSListBucketsOperator' from 'airflow.providers.google.cloud.operators.gcs' (/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/google/cloud/operators/gcs.py)
[2024-08-25T11:53:32.345+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:53:32.384+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.083 seconds
[2024-08-25T11:54:02.495+0000] {processor.py:186} INFO - Started process (PID=6601) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:54:02.496+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:54:02.497+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:54:02.497+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:54:02.516+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:54:02.511+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 3, in <module>
    from airflow.providers.google.cloud.operators.gcs import GCSListBucketsOperator
ImportError: cannot import name 'GCSListBucketsOperator' from 'airflow.providers.google.cloud.operators.gcs' (/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/google/cloud/operators/gcs.py)
[2024-08-25T11:54:02.519+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:54:02.549+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.064 seconds
[2024-08-25T11:54:32.642+0000] {processor.py:186} INFO - Started process (PID=6613) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:54:32.644+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:54:32.645+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:54:32.645+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:54:32.683+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:54:32.678+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 3, in <module>
    from airflow.providers.google.cloud.operators.gcs import GCSListBucketsOperator
ImportError: cannot import name 'GCSListBucketsOperator' from 'airflow.providers.google.cloud.operators.gcs' (/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/google/cloud/operators/gcs.py)
[2024-08-25T11:54:32.686+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:54:32.713+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.077 seconds
[2024-08-25T11:55:02.882+0000] {processor.py:186} INFO - Started process (PID=6625) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:55:02.883+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:55:02.885+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:55:02.884+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:55:02.920+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:55:02.912+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 3, in <module>
    from airflow.providers.google.cloud.operators.gcs import GCSListBucketsOperator
ImportError: cannot import name 'GCSListBucketsOperator' from 'airflow.providers.google.cloud.operators.gcs' (/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/google/cloud/operators/gcs.py)
[2024-08-25T11:55:02.923+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:55:02.949+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.073 seconds
[2024-08-25T11:55:33.134+0000] {processor.py:186} INFO - Started process (PID=6637) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:55:33.136+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:55:33.139+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:55:33.138+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:55:33.162+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:55:33.156+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 3, in <module>
    from airflow.providers.google.cloud.operators.gcs import GCSListBucketsOperator
ImportError: cannot import name 'GCSListBucketsOperator' from 'airflow.providers.google.cloud.operators.gcs' (/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/google/cloud/operators/gcs.py)
[2024-08-25T11:55:33.165+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:55:33.191+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.065 seconds
[2024-08-25T11:56:03.296+0000] {processor.py:186} INFO - Started process (PID=6648) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:56:03.299+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:56:03.302+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:56:03.301+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:56:03.332+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:56:03.323+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 3, in <module>
    from airflow.providers.google.cloud.operators.gcs import GCSListBucketsOperator
ImportError: cannot import name 'GCSListBucketsOperator' from 'airflow.providers.google.cloud.operators.gcs' (/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/google/cloud/operators/gcs.py)
[2024-08-25T11:56:03.336+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:56:03.359+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.072 seconds
[2024-08-25T11:56:33.587+0000] {processor.py:186} INFO - Started process (PID=6666) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:56:33.589+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:56:33.592+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:56:33.591+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:56:33.620+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:56:33.612+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 3, in <module>
    from airflow.providers.google.cloud.operators.gcs import GCSListBucketsOperator
ImportError: cannot import name 'GCSListBucketsOperator' from 'airflow.providers.google.cloud.operators.gcs' (/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/google/cloud/operators/gcs.py)
[2024-08-25T11:56:33.623+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:56:33.663+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.088 seconds
[2024-08-25T11:57:03.896+0000] {processor.py:186} INFO - Started process (PID=6678) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:57:03.898+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:57:03.900+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:57:03.900+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:57:03.962+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:57:03.947+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 3, in <module>
    from airflow.providers.google.cloud.operators.gcs import GCSListBucketsOperator
ImportError: cannot import name 'GCSListBucketsOperator' from 'airflow.providers.google.cloud.operators.gcs' (/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/google/cloud/operators/gcs.py)
[2024-08-25T11:57:03.967+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:57:04.007+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.119 seconds
[2024-08-25T11:57:34.088+0000] {processor.py:186} INFO - Started process (PID=6688) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:57:34.089+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:57:34.092+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:57:34.091+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:57:34.143+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:57:34.132+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 3, in <module>
    from airflow.providers.google.cloud.operators.gcs import GCSListBucketsOperator
ImportError: cannot import name 'GCSListBucketsOperator' from 'airflow.providers.google.cloud.operators.gcs' (/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/google/cloud/operators/gcs.py)
[2024-08-25T11:57:34.147+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:57:34.177+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.100 seconds
[2024-08-25T11:58:04.441+0000] {processor.py:186} INFO - Started process (PID=6700) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:58:04.443+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:58:04.445+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:58:04.445+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:58:04.482+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:58:04.474+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 3, in <module>
    from airflow.providers.google.cloud.operators.gcs import GCSListBucketsOperator
ImportError: cannot import name 'GCSListBucketsOperator' from 'airflow.providers.google.cloud.operators.gcs' (/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/google/cloud/operators/gcs.py)
[2024-08-25T11:58:04.488+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:58:04.514+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.082 seconds
[2024-08-25T11:58:34.640+0000] {processor.py:186} INFO - Started process (PID=6712) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:58:34.641+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:58:34.644+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:58:34.644+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:58:34.696+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:58:34.684+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 3, in <module>
    from airflow.providers.google.cloud.operators.gcs import GCSListBucketsOperator
ImportError: cannot import name 'GCSListBucketsOperator' from 'airflow.providers.google.cloud.operators.gcs' (/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/google/cloud/operators/gcs.py)
[2024-08-25T11:58:34.700+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:58:34.721+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.086 seconds
[2024-08-25T11:59:04.836+0000] {processor.py:186} INFO - Started process (PID=6724) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:59:04.837+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:59:04.839+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:59:04.839+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:59:04.873+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:59:04.864+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 3, in <module>
    from airflow.providers.google.cloud.operators.gcs import GCSListBucketsOperator
ImportError: cannot import name 'GCSListBucketsOperator' from 'airflow.providers.google.cloud.operators.gcs' (/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/google/cloud/operators/gcs.py)
[2024-08-25T11:59:04.876+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:59:04.900+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.071 seconds
[2024-08-25T11:59:35.282+0000] {processor.py:186} INFO - Started process (PID=6736) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:59:35.284+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T11:59:35.286+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:59:35.286+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:59:35.337+0000] {logging_mixin.py:190} INFO - [2024-08-25T11:59:35.324+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 3, in <module>
    from airflow.providers.google.cloud.operators.gcs import GCSListBucketsOperator
ImportError: cannot import name 'GCSListBucketsOperator' from 'airflow.providers.google.cloud.operators.gcs' (/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/google/cloud/operators/gcs.py)
[2024-08-25T11:59:35.341+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T11:59:35.366+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.092 seconds
[2024-08-25T12:00:05.491+0000] {processor.py:186} INFO - Started process (PID=6748) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:00:05.493+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T12:00:05.495+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:00:05.494+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:00:05.553+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:00:05.541+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 3, in <module>
    from airflow.providers.google.cloud.operators.gcs import GCSListBucketsOperator
ImportError: cannot import name 'GCSListBucketsOperator' from 'airflow.providers.google.cloud.operators.gcs' (/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/google/cloud/operators/gcs.py)
[2024-08-25T12:00:05.557+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:00:05.582+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.099 seconds
[2024-08-25T12:00:35.827+0000] {processor.py:186} INFO - Started process (PID=6760) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:00:35.829+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T12:00:35.830+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:00:35.830+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:00:35.881+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:00:35.868+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 3, in <module>
    from airflow.providers.google.cloud.operators.gcs import GCSListBucketsOperator
ImportError: cannot import name 'GCSListBucketsOperator' from 'airflow.providers.google.cloud.operators.gcs' (/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/google/cloud/operators/gcs.py)
[2024-08-25T12:00:35.885+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:00:35.917+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.095 seconds
[2024-08-25T12:01:06.198+0000] {processor.py:186} INFO - Started process (PID=6772) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:01:06.200+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T12:01:06.202+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:01:06.202+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:01:06.234+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:01:06.229+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 3, in <module>
    from airflow.providers.google.cloud.operators.gcs import GCSListBucketsOperator
ImportError: cannot import name 'GCSListBucketsOperator' from 'airflow.providers.google.cloud.operators.gcs' (/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/google/cloud/operators/gcs.py)
[2024-08-25T12:01:06.237+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:01:06.271+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.084 seconds
[2024-08-25T12:01:36.457+0000] {processor.py:186} INFO - Started process (PID=6784) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:01:36.459+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T12:01:36.461+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:01:36.460+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:01:36.496+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:01:36.488+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 3, in <module>
    from airflow.providers.google.cloud.operators.gcs import GCSListBucketsOperator
ImportError: cannot import name 'GCSListBucketsOperator' from 'airflow.providers.google.cloud.operators.gcs' (/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/google/cloud/operators/gcs.py)
[2024-08-25T12:01:36.500+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:01:36.526+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.087 seconds
[2024-08-25T12:02:06.747+0000] {processor.py:186} INFO - Started process (PID=6796) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:02:06.749+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T12:02:06.752+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:02:06.752+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:02:06.820+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:02:06.804+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 3, in <module>
    from airflow.providers.google.cloud.operators.gcs import GCSListBucketsOperator
ImportError: cannot import name 'GCSListBucketsOperator' from 'airflow.providers.google.cloud.operators.gcs' (/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/google/cloud/operators/gcs.py)
[2024-08-25T12:02:06.823+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:02:06.858+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.119 seconds
[2024-08-25T12:02:37.045+0000] {processor.py:186} INFO - Started process (PID=6808) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:02:37.047+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T12:02:37.048+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:02:37.048+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:02:37.102+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:02:37.091+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 3, in <module>
    from airflow.providers.google.cloud.operators.gcs import GCSListBucketsOperator
ImportError: cannot import name 'GCSListBucketsOperator' from 'airflow.providers.google.cloud.operators.gcs' (/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/google/cloud/operators/gcs.py)
[2024-08-25T12:02:37.106+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:02:37.129+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.087 seconds
[2024-08-25T12:03:07.442+0000] {processor.py:186} INFO - Started process (PID=6820) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:03:07.446+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T12:03:07.450+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:03:07.450+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:03:07.481+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:03:07.474+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 3, in <module>
    from airflow.providers.google.cloud.operators.gcs import GCSListBucketsOperator
ImportError: cannot import name 'GCSListBucketsOperator' from 'airflow.providers.google.cloud.operators.gcs' (/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/google/cloud/operators/gcs.py)
[2024-08-25T12:03:07.483+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:03:07.517+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.081 seconds
[2024-08-25T12:03:14.816+0000] {processor.py:186} INFO - Started process (PID=6823) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:03:14.817+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T12:03:14.819+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:03:14.819+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:03:14.869+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:03:14.869+0000] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2024-08-25T12:03:14.891+0000] {processor.py:925} INFO - DAG(s) 'example_gcp_integration' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:03:15.034+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:03:15.034+0000] {override.py:1858} INFO - Created Permission View: can edit on DAG:example_gcp_integration
[2024-08-25T12:03:15.044+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:03:15.044+0000] {override.py:1858} INFO - Created Permission View: can delete on DAG:example_gcp_integration
[2024-08-25T12:03:15.051+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:03:15.051+0000] {override.py:1858} INFO - Created Permission View: can read on DAG:example_gcp_integration
[2024-08-25T12:03:15.052+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:03:15.052+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T12:03:15.063+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:03:15.063+0000] {dag.py:3234} INFO - Creating ORM DAG for example_gcp_integration
[2024-08-25T12:03:15.073+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:03:15.073+0000] {dag.py:4138} INFO - Setting next_dagrun for example_gcp_integration to 2024-08-24 00:00:00+00:00, run_after=2024-08-25 00:00:00+00:00
[2024-08-25T12:03:15.095+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.284 seconds
[2024-08-25T12:03:45.146+0000] {processor.py:186} INFO - Started process (PID=6835) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:03:45.147+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T12:03:45.148+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:03:45.148+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:03:45.189+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:03:45.189+0000] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2024-08-25T12:03:45.236+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:03:45.215+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 18, in <module>
    session.commit()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1454, in commit
    self._transaction.commit(_to_root=self.future)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 832, in commit
    self._prepare_impl()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 811, in _prepare_impl
    self.session.flush()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3449, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3588, in _flush
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3549, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 456, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    _emit_insert_statements(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 1238, in _emit_insert_statements
    result = connection._execute_20(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
[2024-08-25T12:03:45.238+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:03:45.240+0000] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 928, in process_file
    DagFileProcessor.update_import_errors(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 648, in update_import_errors
    existing_import_error_files = [x.filename for x in session.query(ParseImportError.filename).all()]
                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
           ^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-08-25T12:04:15.928+0000] {processor.py:186} INFO - Started process (PID=6847) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:04:15.930+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T12:04:15.933+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:04:15.932+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:04:15.972+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:04:15.971+0000] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2024-08-25T12:04:16.017+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:04:15.996+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 18, in <module>
    session.commit()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1454, in commit
    self._transaction.commit(_to_root=self.future)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 832, in commit
    self._prepare_impl()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 811, in _prepare_impl
    self.session.flush()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3449, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3588, in _flush
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3549, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 456, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    _emit_insert_statements(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 1238, in _emit_insert_statements
    result = connection._execute_20(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
[2024-08-25T12:04:16.022+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:04:16.023+0000] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 928, in process_file
    DagFileProcessor.update_import_errors(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 648, in update_import_errors
    existing_import_error_files = [x.filename for x in session.query(ParseImportError.filename).all()]
                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
           ^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-08-25T12:04:47.022+0000] {processor.py:186} INFO - Started process (PID=6859) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:04:47.024+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T12:04:47.026+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:04:47.025+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:04:47.056+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:04:47.056+0000] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2024-08-25T12:04:47.097+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:04:47.081+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 18, in <module>
    session.commit()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1454, in commit
    self._transaction.commit(_to_root=self.future)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 832, in commit
    self._prepare_impl()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 811, in _prepare_impl
    self.session.flush()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3449, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3588, in _flush
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3549, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 456, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    _emit_insert_statements(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 1238, in _emit_insert_statements
    result = connection._execute_20(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
[2024-08-25T12:04:47.102+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:04:47.103+0000] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 928, in process_file
    DagFileProcessor.update_import_errors(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 648, in update_import_errors
    existing_import_error_files = [x.filename for x in session.query(ParseImportError.filename).all()]
                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
           ^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-08-25T12:05:18.014+0000] {processor.py:186} INFO - Started process (PID=6871) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:05:18.016+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T12:05:18.021+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:05:18.020+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:05:18.094+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:05:18.094+0000] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2024-08-25T12:05:18.130+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:05:18.112+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 18, in <module>
    session.commit()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1454, in commit
    self._transaction.commit(_to_root=self.future)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 832, in commit
    self._prepare_impl()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 811, in _prepare_impl
    self.session.flush()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3449, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3588, in _flush
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3549, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 456, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    _emit_insert_statements(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 1238, in _emit_insert_statements
    result = connection._execute_20(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
[2024-08-25T12:05:18.133+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:05:18.135+0000] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 928, in process_file
    DagFileProcessor.update_import_errors(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 648, in update_import_errors
    existing_import_error_files = [x.filename for x in session.query(ParseImportError.filename).all()]
                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
           ^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-08-25T12:05:48.259+0000] {processor.py:186} INFO - Started process (PID=6883) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:05:48.261+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T12:05:48.263+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:05:48.263+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:05:48.316+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:05:48.316+0000] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2024-08-25T12:05:48.355+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:05:48.336+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 18, in <module>
    session.commit()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1454, in commit
    self._transaction.commit(_to_root=self.future)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 832, in commit
    self._prepare_impl()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 811, in _prepare_impl
    self.session.flush()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3449, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3588, in _flush
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3549, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 456, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    _emit_insert_statements(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 1238, in _emit_insert_statements
    result = connection._execute_20(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
[2024-08-25T12:05:48.359+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:05:48.361+0000] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 928, in process_file
    DagFileProcessor.update_import_errors(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 648, in update_import_errors
    existing_import_error_files = [x.filename for x in session.query(ParseImportError.filename).all()]
                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
           ^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-08-25T12:06:18.943+0000] {processor.py:186} INFO - Started process (PID=6895) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:06:18.945+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T12:06:18.947+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:06:18.946+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:06:18.987+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:06:18.987+0000] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2024-08-25T12:06:19.015+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:06:19.003+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 18, in <module>
    session.commit()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1454, in commit
    self._transaction.commit(_to_root=self.future)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 832, in commit
    self._prepare_impl()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 811, in _prepare_impl
    self.session.flush()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3449, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3588, in _flush
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3549, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 456, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    _emit_insert_statements(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 1238, in _emit_insert_statements
    result = connection._execute_20(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
[2024-08-25T12:06:19.017+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:06:19.019+0000] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 928, in process_file
    DagFileProcessor.update_import_errors(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 648, in update_import_errors
    existing_import_error_files = [x.filename for x in session.query(ParseImportError.filename).all()]
                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
           ^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-08-25T12:06:49.722+0000] {processor.py:186} INFO - Started process (PID=6907) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:06:49.724+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T12:06:49.727+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:06:49.727+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:06:49.774+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:06:49.774+0000] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2024-08-25T12:06:49.804+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:06:49.790+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 18, in <module>
    session.commit()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1454, in commit
    self._transaction.commit(_to_root=self.future)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 832, in commit
    self._prepare_impl()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 811, in _prepare_impl
    self.session.flush()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3449, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3588, in _flush
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3549, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 456, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    _emit_insert_statements(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 1238, in _emit_insert_statements
    result = connection._execute_20(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
[2024-08-25T12:06:49.807+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:06:49.809+0000] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 928, in process_file
    DagFileProcessor.update_import_errors(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 648, in update_import_errors
    existing_import_error_files = [x.filename for x in session.query(ParseImportError.filename).all()]
                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
           ^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-08-25T12:07:20.010+0000] {processor.py:186} INFO - Started process (PID=6919) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:07:20.012+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T12:07:20.014+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:07:20.013+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:07:20.045+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:07:20.044+0000] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2024-08-25T12:07:20.077+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:07:20.064+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 18, in <module>
    session.commit()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1454, in commit
    self._transaction.commit(_to_root=self.future)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 832, in commit
    self._prepare_impl()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 811, in _prepare_impl
    self.session.flush()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3449, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3588, in _flush
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3549, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 456, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    _emit_insert_statements(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 1238, in _emit_insert_statements
    result = connection._execute_20(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
[2024-08-25T12:07:20.080+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:07:20.081+0000] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 928, in process_file
    DagFileProcessor.update_import_errors(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 648, in update_import_errors
    existing_import_error_files = [x.filename for x in session.query(ParseImportError.filename).all()]
                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
           ^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-08-25T12:07:50.193+0000] {processor.py:186} INFO - Started process (PID=6931) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:07:50.194+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T12:07:50.195+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:07:50.195+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:07:50.220+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:07:50.220+0000] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2024-08-25T12:07:50.245+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:07:50.234+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 18, in <module>
    session.commit()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1454, in commit
    self._transaction.commit(_to_root=self.future)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 832, in commit
    self._prepare_impl()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 811, in _prepare_impl
    self.session.flush()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3449, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3588, in _flush
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3549, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 456, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    _emit_insert_statements(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 1238, in _emit_insert_statements
    result = connection._execute_20(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
[2024-08-25T12:07:50.249+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:07:50.250+0000] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 928, in process_file
    DagFileProcessor.update_import_errors(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 648, in update_import_errors
    existing_import_error_files = [x.filename for x in session.query(ParseImportError.filename).all()]
                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
           ^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-08-25T12:08:21.053+0000] {processor.py:186} INFO - Started process (PID=6943) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:08:21.055+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T12:08:21.056+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:08:21.056+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:08:21.101+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:08:21.101+0000] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2024-08-25T12:08:21.127+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:08:21.116+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 18, in <module>
    session.commit()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1454, in commit
    self._transaction.commit(_to_root=self.future)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 832, in commit
    self._prepare_impl()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 811, in _prepare_impl
    self.session.flush()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3449, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3588, in _flush
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3549, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 456, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    _emit_insert_statements(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 1238, in _emit_insert_statements
    result = connection._execute_20(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
[2024-08-25T12:08:21.129+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:08:21.130+0000] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 928, in process_file
    DagFileProcessor.update_import_errors(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 648, in update_import_errors
    existing_import_error_files = [x.filename for x in session.query(ParseImportError.filename).all()]
                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
           ^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-08-25T12:08:52.065+0000] {processor.py:186} INFO - Started process (PID=6956) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:08:52.067+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T12:08:52.068+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:08:52.068+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:08:52.102+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:08:52.102+0000] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2024-08-25T12:08:52.128+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:08:52.118+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 18, in <module>
    session.commit()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1454, in commit
    self._transaction.commit(_to_root=self.future)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 832, in commit
    self._prepare_impl()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 811, in _prepare_impl
    self.session.flush()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3449, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3588, in _flush
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3549, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 456, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    _emit_insert_statements(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 1238, in _emit_insert_statements
    result = connection._execute_20(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
[2024-08-25T12:08:52.130+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:08:52.131+0000] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 928, in process_file
    DagFileProcessor.update_import_errors(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 648, in update_import_errors
    existing_import_error_files = [x.filename for x in session.query(ParseImportError.filename).all()]
                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
           ^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-08-25T12:09:22.203+0000] {processor.py:186} INFO - Started process (PID=6968) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:09:22.205+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T12:09:22.207+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:09:22.206+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:09:22.240+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:09:22.240+0000] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2024-08-25T12:09:22.288+0000] {processor.py:925} INFO - DAG(s) 'example_gcp_integration' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:09:22.437+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:09:22.436+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T12:09:22.465+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:09:22.465+0000] {dag.py:4138} INFO - Setting next_dagrun for example_gcp_integration to 2024-08-24 00:00:00+00:00, run_after=2024-08-25 00:00:00+00:00
[2024-08-25T12:09:22.501+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.309 seconds
[2024-08-25T12:09:52.701+0000] {processor.py:186} INFO - Started process (PID=6980) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:09:52.702+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T12:09:52.704+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:09:52.704+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:09:52.756+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:09:52.755+0000] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2024-08-25T12:09:52.801+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:09:52.784+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 18, in <module>
    session.commit()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1454, in commit
    self._transaction.commit(_to_root=self.future)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 832, in commit
    self._prepare_impl()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 811, in _prepare_impl
    self.session.flush()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3449, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3588, in _flush
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3549, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 456, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    _emit_insert_statements(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 1238, in _emit_insert_statements
    result = connection._execute_20(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
[2024-08-25T12:09:52.804+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:09:52.805+0000] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 928, in process_file
    DagFileProcessor.update_import_errors(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 648, in update_import_errors
    existing_import_error_files = [x.filename for x in session.query(ParseImportError.filename).all()]
                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
           ^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-08-25T12:10:22.960+0000] {processor.py:186} INFO - Started process (PID=6992) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:10:22.964+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T12:10:22.973+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:10:22.971+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:10:23.137+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:10:23.135+0000] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2024-08-25T12:10:23.425+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:10:23.306+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 18, in <module>
    session.commit()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1454, in commit
    self._transaction.commit(_to_root=self.future)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 832, in commit
    self._prepare_impl()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 811, in _prepare_impl
    self.session.flush()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3449, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3588, in _flush
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3549, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 456, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    _emit_insert_statements(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 1238, in _emit_insert_statements
    result = connection._execute_20(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
[2024-08-25T12:10:23.458+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:10:23.473+0000] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 928, in process_file
    DagFileProcessor.update_import_errors(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 648, in update_import_errors
    existing_import_error_files = [x.filename for x in session.query(ParseImportError.filename).all()]
                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
           ^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-08-25T12:10:53.694+0000] {processor.py:186} INFO - Started process (PID=7004) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:10:53.696+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T12:10:53.698+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:10:53.697+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:10:53.748+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:10:53.748+0000] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2024-08-25T12:10:53.785+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:10:53.764+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 18, in <module>
    session.commit()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1454, in commit
    self._transaction.commit(_to_root=self.future)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 832, in commit
    self._prepare_impl()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 811, in _prepare_impl
    self.session.flush()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3449, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3588, in _flush
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3549, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 456, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    _emit_insert_statements(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 1238, in _emit_insert_statements
    result = connection._execute_20(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
[2024-08-25T12:10:53.789+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:10:53.792+0000] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 928, in process_file
    DagFileProcessor.update_import_errors(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 648, in update_import_errors
    existing_import_error_files = [x.filename for x in session.query(ParseImportError.filename).all()]
                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
           ^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-08-25T12:11:24.013+0000] {processor.py:186} INFO - Started process (PID=7015) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:11:24.015+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T12:11:24.016+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:11:24.016+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:11:24.068+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:11:24.067+0000] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2024-08-25T12:11:24.105+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:11:24.089+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 18, in <module>
    session.commit()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1454, in commit
    self._transaction.commit(_to_root=self.future)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 832, in commit
    self._prepare_impl()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 811, in _prepare_impl
    self.session.flush()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3449, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3588, in _flush
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3549, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 456, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    _emit_insert_statements(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 1238, in _emit_insert_statements
    result = connection._execute_20(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
[2024-08-25T12:11:24.107+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:11:24.108+0000] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 928, in process_file
    DagFileProcessor.update_import_errors(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 648, in update_import_errors
    existing_import_error_files = [x.filename for x in session.query(ParseImportError.filename).all()]
                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
           ^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-08-25T12:11:54.471+0000] {processor.py:186} INFO - Started process (PID=7027) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:11:54.490+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T12:11:54.504+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:11:54.497+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:11:54.693+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:11:54.690+0000] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2024-08-25T12:11:54.889+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:11:54.792+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 18, in <module>
    session.commit()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1454, in commit
    self._transaction.commit(_to_root=self.future)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 832, in commit
    self._prepare_impl()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 811, in _prepare_impl
    self.session.flush()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3449, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3588, in _flush
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3549, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 456, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    _emit_insert_statements(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 1238, in _emit_insert_statements
    result = connection._execute_20(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
[2024-08-25T12:11:54.907+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:11:54.912+0000] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 928, in process_file
    DagFileProcessor.update_import_errors(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 648, in update_import_errors
    existing_import_error_files = [x.filename for x in session.query(ParseImportError.filename).all()]
                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
           ^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-08-25T12:12:24.347+0000] {processor.py:186} INFO - Started process (PID=7039) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:12:24.348+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T12:12:24.350+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:12:24.350+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:12:24.388+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:12:24.387+0000] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2024-08-25T12:12:24.411+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:12:24.402+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 18, in <module>
    session.commit()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1454, in commit
    self._transaction.commit(_to_root=self.future)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 832, in commit
    self._prepare_impl()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 811, in _prepare_impl
    self.session.flush()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3449, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3588, in _flush
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3549, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 456, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    _emit_insert_statements(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 1238, in _emit_insert_statements
    result = connection._execute_20(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
[2024-08-25T12:12:24.413+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:12:24.415+0000] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 928, in process_file
    DagFileProcessor.update_import_errors(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 648, in update_import_errors
    existing_import_error_files = [x.filename for x in session.query(ParseImportError.filename).all()]
                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
           ^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-08-25T12:12:38.808+0000] {processor.py:186} INFO - Started process (PID=7046) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:12:38.809+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T12:12:38.811+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:12:38.810+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:12:38.859+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:12:38.859+0000] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2024-08-25T12:12:38.891+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:12:38.876+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 18, in <module>
    session.commit()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1454, in commit
    self._transaction.commit(_to_root=self.future)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 832, in commit
    self._prepare_impl()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 811, in _prepare_impl
    self.session.flush()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3449, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3588, in _flush
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3549, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 456, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    _emit_insert_statements(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 1238, in _emit_insert_statements
    result = connection._execute_20(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
[2024-08-25T12:12:38.894+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:12:38.896+0000] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 928, in process_file
    DagFileProcessor.update_import_errors(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 648, in update_import_errors
    existing_import_error_files = [x.filename for x in session.query(ParseImportError.filename).all()]
                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
           ^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-08-25T12:13:09.861+0000] {processor.py:186} INFO - Started process (PID=7058) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:13:09.863+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T12:13:09.865+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:13:09.865+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:13:09.895+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:13:09.895+0000] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2024-08-25T12:13:09.923+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:13:09.911+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 18, in <module>
    session.commit()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1454, in commit
    self._transaction.commit(_to_root=self.future)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 832, in commit
    self._prepare_impl()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 811, in _prepare_impl
    self.session.flush()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3449, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3588, in _flush
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3549, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 456, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    _emit_insert_statements(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 1238, in _emit_insert_statements
    result = connection._execute_20(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
[2024-08-25T12:13:09.925+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:13:09.927+0000] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 928, in process_file
    DagFileProcessor.update_import_errors(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 648, in update_import_errors
    existing_import_error_files = [x.filename for x in session.query(ParseImportError.filename).all()]
                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
           ^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-08-25T12:13:40.091+0000] {processor.py:186} INFO - Started process (PID=7070) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:13:40.093+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T12:13:40.095+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:13:40.095+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:13:40.170+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:13:40.170+0000] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2024-08-25T12:13:40.199+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:13:40.187+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 18, in <module>
    session.commit()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1454, in commit
    self._transaction.commit(_to_root=self.future)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 832, in commit
    self._prepare_impl()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 811, in _prepare_impl
    self.session.flush()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3449, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3588, in _flush
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3549, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 456, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    _emit_insert_statements(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 1238, in _emit_insert_statements
    result = connection._execute_20(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
[2024-08-25T12:13:40.201+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:13:40.202+0000] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 928, in process_file
    DagFileProcessor.update_import_errors(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 648, in update_import_errors
    existing_import_error_files = [x.filename for x in session.query(ParseImportError.filename).all()]
                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
           ^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-08-25T12:14:04.277+0000] {processor.py:186} INFO - Started process (PID=7076) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:14:04.280+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T12:14:04.282+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:14:04.282+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:14:04.364+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:14:04.364+0000] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2024-08-25T12:14:04.402+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:14:04.382+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 18, in <module>
    session.commit()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1454, in commit
    self._transaction.commit(_to_root=self.future)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 832, in commit
    self._prepare_impl()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 811, in _prepare_impl
    self.session.flush()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3449, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3588, in _flush
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3549, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 456, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    _emit_insert_statements(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 1238, in _emit_insert_statements
    result = connection._execute_20(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
[2024-08-25T12:14:04.405+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:14:04.407+0000] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 928, in process_file
    DagFileProcessor.update_import_errors(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 648, in update_import_errors
    existing_import_error_files = [x.filename for x in session.query(ParseImportError.filename).all()]
                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
           ^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-08-25T12:14:34.647+0000] {processor.py:186} INFO - Started process (PID=7088) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:14:34.648+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T12:14:34.649+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:14:34.649+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:14:34.668+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:14:34.668+0000] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2024-08-25T12:14:34.692+0000] {processor.py:925} INFO - DAG(s) 'example_gcp_integration' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:14:34.787+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:14:34.787+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T12:14:34.809+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:14:34.809+0000] {dag.py:4138} INFO - Setting next_dagrun for example_gcp_integration to 2024-08-25 00:00:00+00:00, run_after=2024-08-26 00:00:00+00:00
[2024-08-25T12:14:34.844+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.204 seconds
[2024-08-25T12:15:05.715+0000] {processor.py:186} INFO - Started process (PID=7110) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:15:05.717+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T12:15:05.719+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:15:05.719+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:15:05.774+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:15:05.774+0000] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2024-08-25T12:15:05.809+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:15:05.792+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 18, in <module>
    session.commit()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1454, in commit
    self._transaction.commit(_to_root=self.future)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 832, in commit
    self._prepare_impl()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 811, in _prepare_impl
    self.session.flush()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3449, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3588, in _flush
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3549, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 456, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    _emit_insert_statements(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 1238, in _emit_insert_statements
    result = connection._execute_20(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
[2024-08-25T12:15:05.811+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:15:05.813+0000] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 928, in process_file
    DagFileProcessor.update_import_errors(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 648, in update_import_errors
    existing_import_error_files = [x.filename for x in session.query(ParseImportError.filename).all()]
                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
           ^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-08-25T12:15:36.659+0000] {processor.py:186} INFO - Started process (PID=7122) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:15:36.662+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T12:15:36.663+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:15:36.663+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:15:36.719+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:15:36.719+0000] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2024-08-25T12:15:36.755+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:15:36.740+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 18, in <module>
    session.commit()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1454, in commit
    self._transaction.commit(_to_root=self.future)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 832, in commit
    self._prepare_impl()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 811, in _prepare_impl
    self.session.flush()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3449, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3588, in _flush
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3549, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 456, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    _emit_insert_statements(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 1238, in _emit_insert_statements
    result = connection._execute_20(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
[2024-08-25T12:15:36.758+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:15:36.759+0000] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 928, in process_file
    DagFileProcessor.update_import_errors(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 648, in update_import_errors
    existing_import_error_files = [x.filename for x in session.query(ParseImportError.filename).all()]
                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
           ^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-08-25T12:16:07.687+0000] {processor.py:186} INFO - Started process (PID=7134) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:16:07.690+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T12:16:07.691+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:16:07.691+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:16:07.749+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:16:07.749+0000] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2024-08-25T12:16:07.785+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:16:07.766+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 18, in <module>
    session.commit()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1454, in commit
    self._transaction.commit(_to_root=self.future)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 832, in commit
    self._prepare_impl()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 811, in _prepare_impl
    self.session.flush()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3449, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3588, in _flush
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3549, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 456, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    _emit_insert_statements(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 1238, in _emit_insert_statements
    result = connection._execute_20(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
[2024-08-25T12:16:07.787+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:16:07.788+0000] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 928, in process_file
    DagFileProcessor.update_import_errors(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 648, in update_import_errors
    existing_import_error_files = [x.filename for x in session.query(ParseImportError.filename).all()]
                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
           ^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-08-25T12:16:37.927+0000] {processor.py:186} INFO - Started process (PID=7146) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:16:37.929+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T12:16:37.931+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:16:37.930+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:16:37.978+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:16:37.977+0000] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2024-08-25T12:16:38.015+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:16:37.999+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 18, in <module>
    session.commit()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1454, in commit
    self._transaction.commit(_to_root=self.future)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 832, in commit
    self._prepare_impl()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 811, in _prepare_impl
    self.session.flush()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3449, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3588, in _flush
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3549, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 456, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    _emit_insert_statements(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 1238, in _emit_insert_statements
    result = connection._execute_20(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
[2024-08-25T12:16:38.019+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:16:38.020+0000] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 928, in process_file
    DagFileProcessor.update_import_errors(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 648, in update_import_errors
    existing_import_error_files = [x.filename for x in session.query(ParseImportError.filename).all()]
                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
           ^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-08-25T12:17:08.372+0000] {processor.py:186} INFO - Started process (PID=7158) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:17:08.374+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T12:17:08.376+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:17:08.376+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:17:08.444+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:17:08.444+0000] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2024-08-25T12:17:08.482+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:17:08.465+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 18, in <module>
    session.commit()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1454, in commit
    self._transaction.commit(_to_root=self.future)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 832, in commit
    self._prepare_impl()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 811, in _prepare_impl
    self.session.flush()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3449, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3588, in _flush
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3549, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 456, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    _emit_insert_statements(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 1238, in _emit_insert_statements
    result = connection._execute_20(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
[2024-08-25T12:17:08.484+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:17:08.486+0000] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 928, in process_file
    DagFileProcessor.update_import_errors(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 648, in update_import_errors
    existing_import_error_files = [x.filename for x in session.query(ParseImportError.filename).all()]
                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
           ^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-08-25T12:17:38.913+0000] {processor.py:186} INFO - Started process (PID=7170) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:17:38.915+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T12:17:38.917+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:17:38.917+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:17:38.968+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:17:38.968+0000] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2024-08-25T12:17:39.011+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:17:38.996+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 18, in <module>
    session.commit()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1454, in commit
    self._transaction.commit(_to_root=self.future)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 832, in commit
    self._prepare_impl()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 811, in _prepare_impl
    self.session.flush()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3449, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3588, in _flush
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3549, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 456, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    _emit_insert_statements(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 1238, in _emit_insert_statements
    result = connection._execute_20(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
[2024-08-25T12:17:39.013+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:17:39.015+0000] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 928, in process_file
    DagFileProcessor.update_import_errors(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 648, in update_import_errors
    existing_import_error_files = [x.filename for x in session.query(ParseImportError.filename).all()]
                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
           ^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-08-25T12:18:10.059+0000] {processor.py:186} INFO - Started process (PID=7182) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:18:10.062+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T12:18:10.065+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:18:10.065+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:18:10.115+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:18:10.114+0000] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2024-08-25T12:18:10.149+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:18:10.134+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 18, in <module>
    session.commit()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1454, in commit
    self._transaction.commit(_to_root=self.future)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 832, in commit
    self._prepare_impl()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 811, in _prepare_impl
    self.session.flush()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3449, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3588, in _flush
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3549, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 456, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    _emit_insert_statements(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 1238, in _emit_insert_statements
    result = connection._execute_20(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
[2024-08-25T12:18:10.153+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:18:10.154+0000] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 928, in process_file
    DagFileProcessor.update_import_errors(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 648, in update_import_errors
    existing_import_error_files = [x.filename for x in session.query(ParseImportError.filename).all()]
                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
           ^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-08-25T12:18:40.347+0000] {processor.py:186} INFO - Started process (PID=7194) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:18:40.349+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T12:18:40.351+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:18:40.351+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:18:40.403+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:18:40.403+0000] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2024-08-25T12:18:40.439+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:18:40.424+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 18, in <module>
    session.commit()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1454, in commit
    self._transaction.commit(_to_root=self.future)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 832, in commit
    self._prepare_impl()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 811, in _prepare_impl
    self.session.flush()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3449, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3588, in _flush
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3549, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 456, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    _emit_insert_statements(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 1238, in _emit_insert_statements
    result = connection._execute_20(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
[2024-08-25T12:18:40.442+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:18:40.444+0000] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 928, in process_file
    DagFileProcessor.update_import_errors(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 648, in update_import_errors
    existing_import_error_files = [x.filename for x in session.query(ParseImportError.filename).all()]
                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
           ^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-08-25T12:19:10.653+0000] {processor.py:186} INFO - Started process (PID=7206) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:19:10.655+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T12:19:10.657+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:19:10.657+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:19:10.702+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:19:10.702+0000] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2024-08-25T12:19:10.738+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:19:10.724+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 18, in <module>
    session.commit()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1454, in commit
    self._transaction.commit(_to_root=self.future)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 832, in commit
    self._prepare_impl()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 811, in _prepare_impl
    self.session.flush()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3449, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3588, in _flush
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3549, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 456, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    _emit_insert_statements(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 1238, in _emit_insert_statements
    result = connection._execute_20(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
[2024-08-25T12:19:10.742+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:19:10.744+0000] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 928, in process_file
    DagFileProcessor.update_import_errors(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 648, in update_import_errors
    existing_import_error_files = [x.filename for x in session.query(ParseImportError.filename).all()]
                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
           ^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-08-25T12:19:41.060+0000] {processor.py:186} INFO - Started process (PID=7218) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:19:41.062+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T12:19:41.064+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:19:41.064+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:19:41.114+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:19:41.114+0000] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2024-08-25T12:19:41.155+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:19:41.138+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 18, in <module>
    session.commit()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1454, in commit
    self._transaction.commit(_to_root=self.future)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 832, in commit
    self._prepare_impl()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 811, in _prepare_impl
    self.session.flush()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3449, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3588, in _flush
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3549, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 456, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    _emit_insert_statements(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 1238, in _emit_insert_statements
    result = connection._execute_20(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
[2024-08-25T12:19:41.159+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:19:41.160+0000] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 928, in process_file
    DagFileProcessor.update_import_errors(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 648, in update_import_errors
    existing_import_error_files = [x.filename for x in session.query(ParseImportError.filename).all()]
                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
           ^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-08-25T12:20:11.691+0000] {processor.py:186} INFO - Started process (PID=7230) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:20:11.695+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T12:20:11.697+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:20:11.697+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:20:11.734+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:20:11.733+0000] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2024-08-25T12:20:11.770+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:20:11.756+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 18, in <module>
    session.commit()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1454, in commit
    self._transaction.commit(_to_root=self.future)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 832, in commit
    self._prepare_impl()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 811, in _prepare_impl
    self.session.flush()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3449, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3588, in _flush
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3549, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 456, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    _emit_insert_statements(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 1238, in _emit_insert_statements
    result = connection._execute_20(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
[2024-08-25T12:20:11.775+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:20:11.776+0000] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 928, in process_file
    DagFileProcessor.update_import_errors(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 648, in update_import_errors
    existing_import_error_files = [x.filename for x in session.query(ParseImportError.filename).all()]
                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
           ^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-08-25T12:20:41.980+0000] {processor.py:186} INFO - Started process (PID=7242) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:20:41.982+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T12:20:41.983+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:20:41.983+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:20:42.030+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:20:42.030+0000] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2024-08-25T12:20:42.052+0000] {processor.py:925} INFO - DAG(s) 'example_gcp_integration' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:20:42.142+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:20:42.142+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T12:20:42.159+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:20:42.159+0000] {dag.py:4138} INFO - Setting next_dagrun for example_gcp_integration to 2024-08-25 00:00:00+00:00, run_after=2024-08-26 00:00:00+00:00
[2024-08-25T12:20:42.188+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.214 seconds
[2024-08-25T12:21:12.302+0000] {processor.py:186} INFO - Started process (PID=7258) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:21:12.305+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T12:21:12.308+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:21:12.307+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:21:12.360+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:21:12.360+0000] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2024-08-25T12:21:12.393+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:21:12.380+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 18, in <module>
    session.commit()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1454, in commit
    self._transaction.commit(_to_root=self.future)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 832, in commit
    self._prepare_impl()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 811, in _prepare_impl
    self.session.flush()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3449, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3588, in _flush
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3549, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 456, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    _emit_insert_statements(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 1238, in _emit_insert_statements
    result = connection._execute_20(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
[2024-08-25T12:21:12.397+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:21:12.398+0000] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 928, in process_file
    DagFileProcessor.update_import_errors(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 648, in update_import_errors
    existing_import_error_files = [x.filename for x in session.query(ParseImportError.filename).all()]
                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
           ^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-08-25T12:21:42.551+0000] {processor.py:186} INFO - Started process (PID=7270) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:21:42.553+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T12:21:42.555+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:21:42.555+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:21:42.584+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:21:42.583+0000] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2024-08-25T12:21:42.604+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:21:42.597+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 18, in <module>
    session.commit()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1454, in commit
    self._transaction.commit(_to_root=self.future)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 832, in commit
    self._prepare_impl()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 811, in _prepare_impl
    self.session.flush()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3449, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3588, in _flush
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3549, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 456, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    _emit_insert_statements(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 1238, in _emit_insert_statements
    result = connection._execute_20(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
[2024-08-25T12:21:42.606+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:21:42.607+0000] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 928, in process_file
    DagFileProcessor.update_import_errors(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 648, in update_import_errors
    existing_import_error_files = [x.filename for x in session.query(ParseImportError.filename).all()]
                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
           ^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-08-25T12:22:12.722+0000] {processor.py:186} INFO - Started process (PID=7282) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:22:12.723+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T12:22:12.725+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:22:12.725+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:22:12.779+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:22:12.777+0000] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2024-08-25T12:22:12.831+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:22:12.813+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 18, in <module>
    session.commit()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1454, in commit
    self._transaction.commit(_to_root=self.future)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 832, in commit
    self._prepare_impl()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 811, in _prepare_impl
    self.session.flush()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3449, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3588, in _flush
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3549, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 456, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    _emit_insert_statements(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 1238, in _emit_insert_statements
    result = connection._execute_20(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
[2024-08-25T12:22:12.835+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:22:12.838+0000] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 928, in process_file
    DagFileProcessor.update_import_errors(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 648, in update_import_errors
    existing_import_error_files = [x.filename for x in session.query(ParseImportError.filename).all()]
                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
           ^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-08-25T12:22:43.334+0000] {processor.py:186} INFO - Started process (PID=7294) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:22:43.335+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T12:22:43.337+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:22:43.337+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:22:43.381+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:22:43.381+0000] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2024-08-25T12:22:43.410+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:22:43.396+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/gcp_connection_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/gcp_connection_test.py", line 18, in <module>
    session.commit()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1454, in commit
    self._transaction.commit(_to_root=self.future)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 832, in commit
    self._prepare_impl()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 811, in _prepare_impl
    self.session.flush()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3449, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3588, in _flush
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3549, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 456, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    _emit_insert_statements(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 1238, in _emit_insert_statements
    result = connection._execute_20(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
[2024-08-25T12:22:43.413+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:22:43.416+0000] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 928, in process_file
    DagFileProcessor.update_import_errors(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 648, in update_import_errors
    existing_import_error_files = [x.filename for x in session.query(ParseImportError.filename).all()]
                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
           ^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "connection_conn_id_uq"
DETAIL:  Key (conn_id)=(google_cloud_default) already exists.

[SQL: INSERT INTO connection (conn_id, conn_type, description, host, schema, login, password, port, is_encrypted, is_extra_encrypted, extra) VALUES (%(conn_id)s, %(conn_type)s, %(description)s, %(host)s, %(schema)s, %(login)s, %(password)s, %(port)s, %(is_encrypted)s, %(is_extra_encrypted)s, %(extra)s) RETURNING connection.id]
[parameters: {'conn_id': 'google_cloud_default', 'conn_type': 'google_cloud_platform', 'description': None, 'host': None, 'schema': None, 'login': None, 'password': None, 'port': None, 'is_encrypted': False, 'is_extra_encrypted': False, 'extra': '{"extra__google_cloud_platform__keyfile_dict":"C:/Users/sst7260/Documents/shreyas/system_keys/aceinternal-2ed449d3-de257b0daf3f.json"}'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-08-25T12:22:51.666+0000] {processor.py:186} INFO - Started process (PID=7295) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:22:51.668+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T12:22:51.670+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:22:51.670+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:22:51.772+0000] {processor.py:925} INFO - DAG(s) 'gcp_integration_dag' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:22:51.909+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:22:51.909+0000] {override.py:1858} INFO - Created Permission View: can edit on DAG:gcp_integration_dag
[2024-08-25T12:22:51.926+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:22:51.925+0000] {override.py:1858} INFO - Created Permission View: can delete on DAG:gcp_integration_dag
[2024-08-25T12:22:51.937+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:22:51.937+0000] {override.py:1858} INFO - Created Permission View: can read on DAG:gcp_integration_dag
[2024-08-25T12:22:51.938+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:22:51.938+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T12:22:51.954+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:22:51.953+0000] {dag.py:3234} INFO - Creating ORM DAG for gcp_integration_dag
[2024-08-25T12:22:51.966+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:22:51.966+0000] {dag.py:4138} INFO - Setting next_dagrun for gcp_integration_dag to 2024-08-24 00:00:00+00:00, run_after=2024-08-25 00:00:00+00:00
[2024-08-25T12:22:52.002+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.343 seconds
[2024-08-25T12:23:02.529+0000] {processor.py:186} INFO - Started process (PID=7301) to work on /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:23:02.530+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/gcp_connection_test.py for tasks to queue
[2024-08-25T12:23:02.532+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:23:02.532+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:23:02.565+0000] {processor.py:925} INFO - DAG(s) 'gcp_integration_dag' retrieved from /opt/airflow/dags/gcp_connection_test.py
[2024-08-25T12:23:02.578+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:23:02.578+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-25T12:23:02.603+0000] {logging_mixin.py:190} INFO - [2024-08-25T12:23:02.603+0000] {dag.py:4138} INFO - Setting next_dagrun for gcp_integration_dag to 2024-08-24 00:00:00+00:00, run_after=2024-08-25 00:00:00+00:00
[2024-08-25T12:23:02.630+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/gcp_connection_test.py took 0.109 seconds
